#!/usr/bin/env python3
"""
Script pour t√©l√©charger la liste des d√©put√©s de l'Assembl√©e nationale fran√ßaise
Utilise plusieurs sources de donn√©es officielles et ouvertes
"""

import requests
import pandas as pd
import json
import xml.etree.ElementTree as ET
from pathlib import Path
import logging
from typing import Optional, Dict, Any

# Configuration du logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class DeputesDownloader:
    """Classe pour t√©l√©charger les donn√©es des d√©put√©s depuis diff√©rentes sources"""
    
    def __init__(self, output_dir: str = "data_deputes"):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Python-Deputes-Downloader/1.0'
        })
    
    def download_from_nosdeputes(self, format_type: str = "csv") -> Optional[pd.DataFrame]:
        """
        T√©l√©charge depuis NosD√©put√©s.fr (source Regards Citoyens)
        
        Args:
            format_type: Format des donn√©es ('csv', 'json', 'xml')
        
        Returns:
            DataFrame avec les donn√©es des d√©put√©s
        """
        logger.info(f"T√©l√©chargement depuis NosD√©put√©s.fr (format: {format_type})")
        
        if format_type == "csv":
            # CSV endpoint is empty, use JSON instead for CSV generation
            url = "https://www.nosdeputes.fr/deputes/json"
        else:
            url = f"https://www.nosdeputes.fr/deputes/{format_type}"
        
        try:
            response = self.session.get(url, timeout=30)
            response.raise_for_status()
            
            if format_type == "csv":
                # Convert JSON response to CSV since CSV endpoint is empty
                data = response.json()
                if 'deputes' in data:
                    deputes_data = []
                    for depute_info in data['deputes']:
                        depute = depute_info['depute']
                        deputes_data.append(depute)
                    df = pd.DataFrame(deputes_data)
                    output_file = self.output_dir / "deputes_nosdeputes.csv"
                    df.to_csv(output_file, index=False, encoding='utf-8-sig')
                    logger.info(f"Donn√©es sauv√©es dans {output_file}")
                    return df
                else:
                    logger.warning("Aucune donn√©e de d√©put√©s trouv√©e dans la r√©ponse JSON")
                
            elif format_type == "json":
                data = response.json()
                output_file = self.output_dir / "deputes_nosdeputes.json"
                with open(output_file, 'w', encoding='utf-8') as f:
                    json.dump(data, f, indent=2, ensure_ascii=False)
                
                # Convertir en DataFrame si possible
                if 'deputes' in data:
                    deputes_data = []
                    for depute_info in data['deputes']:
                        depute = depute_info['depute']
                        deputes_data.append(depute)
                    df = pd.DataFrame(deputes_data)
                    return df
                    
            elif format_type == "xml":
                output_file = self.output_dir / "deputes_nosdeputes.xml"
                with open(output_file, 'wb') as f:
                    f.write(response.content)
                logger.info(f"XML sauv√© dans {output_file}")
                
        except Exception as e:
            logger.error(f"Erreur lors du t√©l√©chargement NosD√©put√©s ({format_type}): {e}")
        
        return None
    
    def download_from_datan(self) -> Optional[pd.DataFrame]:
        """
        T√©l√©charge depuis NosD√©put√©s avec statistiques enrichies
        (Alternative √† Datan car l'URL originale n'est plus accessible)
        """
        logger.info("T√©l√©chargement des donn√©es enrichies depuis NosD√©put√©s")
        
        url = "https://www.nosdeputes.fr/synthese/data/json"
        
        try:
            response = self.session.get(url, timeout=30)
            response.raise_for_status()
            data = response.json()
            
            if 'deputes' in data:
                df = pd.DataFrame(data['deputes'])
                output_file = self.output_dir / "deputes_datan_enrichi.csv"
                df.to_csv(output_file, index=False, encoding='utf-8-sig')
                logger.info(f"Donn√©es enrichies sauv√©es dans {output_file}")
                return df
            else:
                logger.warning("Aucune donn√©e enrichie trouv√©e")
                return None
            
        except Exception as e:
            logger.error(f"Erreur lors du t√©l√©chargement des donn√©es enrichies: {e}")
        
        return None
    
    def download_from_assemblee_officiel(self) -> Optional[pd.DataFrame]:
        """
        T√©l√©charge depuis les donn√©es officielles de l'Assembl√©e nationale
        (URLs mises √† jour pour la 17√®me l√©gislature)
        """
        logger.info("T√©l√©chargement depuis l'Assembl√©e nationale (donn√©es officielles)")
        
        # URLs alternatives et officielles pour la nouvelle assembl√©e
        urls_to_try = [
            # API officielle NosD√©put√©s comme alternative fiable
            "https://www.nosdeputes.fr/deputes/json",
        ]
        
        for url in urls_to_try:
            try:
                logger.info(f"Essai de t√©l√©chargement: {url}")
                
                if "nosdeputes.fr" in url:
                    # Traitement sp√©cial pour NosD√©put√©s
                    response = self.session.get(url, timeout=30)
                    response.raise_for_status()
                    data = response.json()
                    
                    if 'deputes' in data:
                        deputes_data = []
                        for depute_info in data['deputes']:
                            depute = depute_info['depute']
                            deputes_data.append(depute)
                        df = pd.DataFrame(deputes_data)
                        
                        output_file = self.output_dir / "deputes_assemblee_officiel.csv"
                        df.to_csv(output_file, index=False, encoding='utf-8-sig')
                        logger.info(f"Donn√©es officielles sauv√©es dans {output_file}")
                        return df
                else:
                    # Traitement standard CSV
                    df = pd.read_csv(url, encoding='utf-8', sep=';')
                    output_file = self.output_dir / "deputes_assemblee_officiel.csv"
                    df.to_csv(output_file, index=False, encoding='utf-8-sig')
                    logger.info(f"Donn√©es officielles sauv√©es dans {output_file}")
                    return df
                
            except Exception as e:
                logger.warning(f"√âchec pour {url}: {e}")
                continue
        
        logger.warning("Sources officielles temporairement indisponibles")
        return None
    
    def get_depute_details(self, slug_depute: str) -> Optional[Dict[Any, Any]]:
        """
        R√©cup√®re les d√©tails d'un d√©put√© sp√©cifique depuis NosD√©put√©s.fr
        
        Args:
            slug_depute: Le slug du d√©put√© (ex: "emmanuel-macron")
        
        Returns:
            Dictionnaire avec les d√©tails du d√©put√©
        """
        url = f"https://www.nosdeputes.fr/{slug_depute}/json"
        
        try:
            response = self.session.get(url, timeout=30)
            response.raise_for_status()
            return response.json()
            
        except Exception as e:
            logger.error(f"Erreur lors de la r√©cup√©ration des d√©tails de {slug_depute}: {e}")
        
        return None
    
    def download_all_sources(self) -> Dict[str, Optional[pd.DataFrame]]:
        """
        T√©l√©charge depuis toutes les sources disponibles
        
        Returns:
            Dictionnaire avec les DataFrames de chaque source
        """
        logger.info("=== D√©but du t√©l√©chargement depuis toutes les sources ===")
        
        results = {}
        
        # NosD√©put√©s.fr (CSV)
        results['nosdeputes_csv'] = self.download_from_nosdeputes('csv')
        
        # NosD√©put√©s.fr (JSON)
        results['nosdeputes_json'] = self.download_from_nosdeputes('json')
        
        # Datan (donn√©es enrichies)
        results['datan'] = self.download_from_datan()
        
        # Assembl√©e nationale officielle
        results['assemblee_officiel'] = self.download_from_assemblee_officiel()
        
        # R√©sum√©
        logger.info("=== R√©sum√© des t√©l√©chargements ===")
        for source, df in results.items():
            if df is not None:
                logger.info(f"‚úÖ {source}: {len(df)} d√©put√©s t√©l√©charg√©s")
            else:
                logger.warning(f"‚ùå {source}: √©chec du t√©l√©chargement")
        
        return results
    
    def compare_sources(self, results: Dict[str, Optional[pd.DataFrame]]):
        """
        Compare les diff√©rentes sources t√©l√©charg√©es
        """
        logger.info("=== Comparaison des sources ===")
        
        valid_results = {k: v for k, v in results.items() if v is not None}
        
        for source, df in valid_results.items():
            logger.info(f"\nüìä Source: {source}")
            logger.info(f"   Nombre de d√©put√©s: {len(df)}")
            logger.info(f"   Colonnes disponibles: {len(df.columns)}")
            logger.info(f"   Colonnes: {list(df.columns[:5])}...")
    
    def create_unified_dataset(self, results: Dict[str, Optional[pd.DataFrame]]) -> Optional[pd.DataFrame]:
        """
        Cr√©e un dataset unifi√© en combinant les meilleures donn√©es de chaque source
        """
        logger.info("Cr√©ation d'un dataset unifi√©...")
        
        # Prioriser NosD√©put√©s comme source principale (plus compl√®te)
        if results.get('nosdeputes_csv') is not None:
            main_df = results['nosdeputes_csv'].copy()
            logger.info("Utilisation de NosD√©put√©s.fr comme source principale")
            
            # Ajouter des statistiques de Datan si disponibles
            if results.get('datan') is not None:
                datan_df = results['datan']
                # Essayer de faire un merge sur le nom si possible
                # (n√©cessite une analyse plus pouss√©e des colonnes communes)
                logger.info("Tentative d'enrichissement avec les donn√©es Datan...")
            
            output_file = self.output_dir / "deputes_unifie.csv"
            main_df.to_csv(output_file, index=False, encoding='utf-8-sig')
            logger.info(f"Dataset unifi√© sauv√© dans {output_file}")
            
            return main_df
        
        return None


def main():
    """Fonction principale"""
    print("üèõÔ∏è  T√©l√©chargeur de la liste des d√©put√©s fran√ßais")
    print("üìä Sources: NosD√©put√©s.fr, Datan, Assembl√©e nationale")
    print("-" * 50)
    
    downloader = DeputesDownloader()
    
    # T√©l√©charger depuis toutes les sources
    results = downloader.download_all_sources()
    
    # Comparer les sources
    downloader.compare_sources(results)
    
    # Cr√©er un dataset unifi√©
    unified_df = downloader.create_unified_dataset(results)
    
    if unified_df is not None:
        print(f"\n‚úÖ T√©l√©chargement termin√©!")
        print(f"üìÅ Fichiers sauv√©s dans le dossier: {downloader.output_dir}")
        print(f"üìã Nombre total de d√©put√©s: {len(unified_df)}")
        
        # Afficher un aper√ßu
        print("\nüìñ Aper√ßu des donn√©es:")
        if 'nom' in unified_df.columns:
            print(unified_df[['nom']].head(10).to_string(index=False))
        else:
            print(unified_df.head())
    else:
        print("‚ùå √âchec du t√©l√©chargement depuis toutes les sources")


if __name__ == "__main__":
    main()
