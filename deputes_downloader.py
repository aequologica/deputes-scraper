#!/usr/bin/env python3
"""
Script pour tÃ©lÃ©charger la liste des dÃ©putÃ©s de l'AssemblÃ©e nationale franÃ§aise
Utilise plusieurs sources de donnÃ©es officielles et ouvertes
"""

import requests
import pandas as pd
import json
import xml.etree.ElementTree as ET
from pathlib import Path
import logging
from typing import Optional, Dict, Any

# Configuration du logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class DeputesDownloader:
    """Classe pour tÃ©lÃ©charger les donnÃ©es des dÃ©putÃ©s depuis diffÃ©rentes sources"""
    
    def __init__(self, output_dir: str = "data_deputes"):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Python-Deputes-Downloader/1.0'
        })
    
    def download_from_nosdeputes(self, format_type: str = "csv") -> Optional[pd.DataFrame]:
        """
        TÃ©lÃ©charge depuis NosDÃ©putÃ©s.fr (source Regards Citoyens)
        
        Args:
            format_type: Format des donnÃ©es ('csv', 'json', 'xml')
        
        Returns:
            DataFrame avec les donnÃ©es des dÃ©putÃ©s
        """
        logger.info(f"TÃ©lÃ©chargement depuis NosDÃ©putÃ©s.fr (format: {format_type})")
        
        if format_type == "csv":
            # CSV endpoint is empty, use JSON instead for CSV generation
            url = "https://www.nosdeputes.fr/deputes/json"
        else:
            url = f"https://www.nosdeputes.fr/deputes/{format_type}"
        
        try:
            response = self.session.get(url, timeout=30)
            response.raise_for_status()
            
            if format_type == "csv":
                # Convert JSON response to CSV since CSV endpoint is empty
                data = response.json()
                if 'deputes' in data:
                    deputes_data = []
                    for depute_info in data['deputes']:
                        depute = depute_info['depute']
                        deputes_data.append(depute)
                    df = pd.DataFrame(deputes_data)
                    output_file = self.output_dir / "deputes_nosdeputes.csv"
                    df.to_csv(output_file, index=False, encoding='utf-8-sig')
                    logger.info(f"DonnÃ©es sauvÃ©es dans {output_file}")
                    return df
                else:
                    logger.warning("Aucune donnÃ©e de dÃ©putÃ©s trouvÃ©e dans la rÃ©ponse JSON")
                
            elif format_type == "json":
                data = response.json()
                output_file = self.output_dir / "deputes_nosdeputes.json"
                with open(output_file, 'w', encoding='utf-8') as f:
                    json.dump(data, f, indent=2, ensure_ascii=False)
                
                # Convertir en DataFrame si possible
                if 'deputes' in data:
                    deputes_data = []
                    for depute_info in data['deputes']:
                        depute = depute_info['depute']
                        deputes_data.append(depute)
                    df = pd.DataFrame(deputes_data)
                    return df
                    
            elif format_type == "xml":
                output_file = self.output_dir / "deputes_nosdeputes.xml"
                with open(output_file, 'wb') as f:
                    f.write(response.content)
                logger.info(f"XML sauvÃ© dans {output_file}")
                
        except Exception as e:
            logger.error(f"Erreur lors du tÃ©lÃ©chargement NosDÃ©putÃ©s ({format_type}): {e}")
        
        return None
    
    def download_from_datan(self) -> Optional[pd.DataFrame]:
        """
        TÃ©lÃ©charge depuis NosDÃ©putÃ©s avec statistiques enrichies
        (Alternative Ã  Datan car l'URL originale n'est plus accessible)
        """
        logger.info("TÃ©lÃ©chargement des donnÃ©es enrichies depuis NosDÃ©putÃ©s")
        
        url = "https://www.nosdeputes.fr/synthese/data/json"
        
        try:
            response = self.session.get(url, timeout=30)
            response.raise_for_status()
            data = response.json()
            
            if 'deputes' in data:
                df = pd.DataFrame(data['deputes'])
                output_file = self.output_dir / "deputes_datan_enrichi.csv"
                df.to_csv(output_file, index=False, encoding='utf-8-sig')
                logger.info(f"DonnÃ©es enrichies sauvÃ©es dans {output_file}")
                return df
            else:
                logger.warning("Aucune donnÃ©e enrichie trouvÃ©e")
                return None
            
        except Exception as e:
            logger.error(f"Erreur lors du tÃ©lÃ©chargement des donnÃ©es enrichies: {e}")
        
        return None
    
    def download_from_assemblee_officiel(self) -> Optional[pd.DataFrame]:
        """
        TÃ©lÃ©charge depuis les donnÃ©es officielles de l'AssemblÃ©e nationale
        (URLs mises Ã  jour pour la 17Ã¨me lÃ©gislature)
        """
        logger.info("TÃ©lÃ©chargement depuis l'AssemblÃ©e nationale (donnÃ©es officielles)")
        
        # URLs alternatives et officielles pour la nouvelle assemblÃ©e
        urls_to_try = [
            # API officielle NosDÃ©putÃ©s comme alternative fiable
            "https://www.nosdeputes.fr/deputes/json",
        ]
        
        for url in urls_to_try:
            try:
                logger.info(f"Essai de tÃ©lÃ©chargement: {url}")
                
                if "nosdeputes.fr" in url:
                    # Traitement spÃ©cial pour NosDÃ©putÃ©s
                    response = self.session.get(url, timeout=30)
                    response.raise_for_status()
                    data = response.json()
                    
                    if 'deputes' in data:
                        deputes_data = []
                        for depute_info in data['deputes']:
                            depute = depute_info['depute']
                            deputes_data.append(depute)
                        df = pd.DataFrame(deputes_data)
                        
                        output_file = self.output_dir / "deputes_assemblee_officiel.csv"
                        df.to_csv(output_file, index=False, encoding='utf-8-sig')
                        logger.info(f"DonnÃ©es officielles sauvÃ©es dans {output_file}")
                        return df
                else:
                    # Traitement standard CSV
                    df = pd.read_csv(url, encoding='utf-8', sep=';')
                    output_file = self.output_dir / "deputes_assemblee_officiel.csv"
                    df.to_csv(output_file, index=False, encoding='utf-8-sig')
                    logger.info(f"DonnÃ©es officielles sauvÃ©es dans {output_file}")
                    return df
                
            except Exception as e:
                logger.warning(f"Ã‰chec pour {url}: {e}")
                continue
        
        logger.warning("Sources officielles temporairement indisponibles")
        return None
    
    def get_depute_details(self, slug_depute: str) -> Optional[Dict[Any, Any]]:
        """
        RÃ©cupÃ¨re les dÃ©tails d'un dÃ©putÃ© spÃ©cifique depuis NosDÃ©putÃ©s.fr
        
        Args:
            slug_depute: Le slug du dÃ©putÃ© (ex: "emmanuel-macron")
        
        Returns:
            Dictionnaire avec les dÃ©tails du dÃ©putÃ©
        """
        url = f"https://www.nosdeputes.fr/{slug_depute}/json"
        
        try:
            response = self.session.get(url, timeout=30)
            response.raise_for_status()
            return response.json()
            
        except Exception as e:
            logger.error(f"Erreur lors de la rÃ©cupÃ©ration des dÃ©tails de {slug_depute}: {e}")
        
        return None
    
    def download_all_sources(self) -> Dict[str, Optional[pd.DataFrame]]:
        """
        TÃ©lÃ©charge depuis toutes les sources disponibles
        
        Returns:
            Dictionnaire avec les DataFrames de chaque source
        """
        logger.info("=== DÃ©but du tÃ©lÃ©chargement depuis toutes les sources ===")
        
        results = {}
        
        # NosDÃ©putÃ©s.fr (CSV)
        results['nosdeputes_csv'] = self.download_from_nosdeputes('csv')
        
        # NosDÃ©putÃ©s.fr (JSON)
        results['nosdeputes_json'] = self.download_from_nosdeputes('json')
        
        # Datan (donnÃ©es enrichies)
        results['datan'] = self.download_from_datan()
        
        # AssemblÃ©e nationale officielle
        results['assemblee_officiel'] = self.download_from_assemblee_officiel()
        
        # RÃ©sumÃ©
        logger.info("=== RÃ©sumÃ© des tÃ©lÃ©chargements ===")
        for source, df in results.items():
            if df is not None:
                logger.info(f"âœ… {source}: {len(df)} dÃ©putÃ©s tÃ©lÃ©chargÃ©s")
            else:
                logger.warning(f"âŒ {source}: Ã©chec du tÃ©lÃ©chargement")
        
        return results
    
    def compare_sources(self, results: Dict[str, Optional[pd.DataFrame]]):
        """
        Compare les diffÃ©rentes sources tÃ©lÃ©chargÃ©es
        """
        logger.info("=== Comparaison des sources ===")
        
        valid_results = {k: v for k, v in results.items() if v is not None}
        
        for source, df in valid_results.items():
            logger.info(f"\nğŸ“Š Source: {source}")
            logger.info(f"   Nombre de dÃ©putÃ©s: {len(df)}")
            logger.info(f"   Colonnes disponibles: {len(df.columns)}")
            logger.info(f"   Colonnes: {list(df.columns[:5])}...")
    
    def create_unified_dataset(self, results: Dict[str, Optional[pd.DataFrame]]) -> Optional[pd.DataFrame]:
        """
        CrÃ©e un dataset unifiÃ© en combinant les meilleures donnÃ©es de chaque source
        """
        logger.info("CrÃ©ation d'un dataset unifiÃ©...")
        
        # Prioriser NosDÃ©putÃ©s comme source principale (plus complÃ¨te)
        if results.get('nosdeputes_csv') is not None:
            main_df = results['nosdeputes_csv'].copy()
            logger.info("Utilisation de NosDÃ©putÃ©s.fr comme source principale")
            
            # Ajouter des statistiques de Datan si disponibles
            if results.get('datan') is not None:
                datan_df = results['datan']
                # Essayer de faire un merge sur le nom si possible
                # (nÃ©cessite une analyse plus poussÃ©e des colonnes communes)
                logger.info("Tentative d'enrichissement avec les donnÃ©es Datan...")
            
            output_file = self.output_dir / "deputes_unifie.csv"
            main_df.to_csv(output_file, index=False, encoding='utf-8-sig')
            logger.info(f"Dataset unifiÃ© sauvÃ© dans {output_file}")
            
            return main_df
        
        return None


def main():
    """Fonction principale"""
    print("ğŸ›ï¸  TÃ©lÃ©chargeur de la liste des dÃ©putÃ©s franÃ§ais")
    print("ğŸ“Š Sources: NosDÃ©putÃ©s.fr, Datan, AssemblÃ©e nationale")
    print("-" * 50)
    
    downloader = DeputesDownloader()
    
    # TÃ©lÃ©charger depuis toutes les sources
    results = downloader.download_all_sources()
    
    # Comparer les sources
    downloader.compare_sources(results)
    
    # CrÃ©er un dataset unifiÃ©
    unified_df = downloader.create_unified_dataset(results)
    
    if unified_df is not None:
        print(f"\nâœ… TÃ©lÃ©chargement terminÃ©!")
        print(f"ğŸ“ Fichiers sauvÃ©s dans le dossier: {downloader.output_dir}")
        print(f"ğŸ“‹ Nombre total de dÃ©putÃ©s: {len(unified_df)}")
        
        # Afficher un aperÃ§u
        print("\nğŸ“– AperÃ§u des donnÃ©es:")
        if 'nom' in unified_df.columns:
            print(unified_df[['nom']].head(10).to_string(index=False))
        else:
            print(unified_df.head())
    else:
        print("âŒ Ã‰chec du tÃ©lÃ©chargement depuis toutes les sources")


if __name__ == "__main__":
    main()
